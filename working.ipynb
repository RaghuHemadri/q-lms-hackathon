{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, file_paths, chunk_size=1000):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader for multiple log files.\n",
    "\n",
    "        :param file_paths: List of log file paths to process.\n",
    "        :param chunk_size: The number of log lines to yield in each chunk.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.chunk_size = chunk_size\n",
    "        self.files = [open(file_path, 'r') for file_path in file_paths]  # Open all files\n",
    "        self.current_file_idx = 0\n",
    "        self.current_file = self.files[self.current_file_idx]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator to process the log files in chunks.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Reads the next chunk of log lines from the current file.\n",
    "\n",
    "        :return: A list of log lines as a chunk.\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        while len(lines) < self.chunk_size:\n",
    "            line = self.current_file.readline()\n",
    "            if not line:  # If end of file is reached, move to next file\n",
    "                self.current_file_idx += 1\n",
    "                if self.current_file_idx >= len(self.files):\n",
    "                    # If we've processed all files, stop iteration\n",
    "                    raise StopIteration\n",
    "                self.current_file = self.files[self.current_file_idx]\n",
    "                continue\n",
    "            lines.append(line.strip())  # Strip newlines and whitespace from each line\n",
    "        return self.current_file_idx, lines\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close all open files when finished.\n",
    "        \"\"\"\n",
    "        for file in self.files:\n",
    "            file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that DataLoader class has been defined as above.\n",
    "\n",
    "# Step 1: Define the log file paths\n",
    "log_dir = \"logs\"\n",
    "\n",
    "log_file_paths = [os.path.join(log_dir, file) for file in os.listdir(log_dir) if file.endswith('.log')]\n",
    "print(log_file_paths)\n",
    "\n",
    "# Step 2: Initialize the DataLoader with the log file paths and chunk size\n",
    "train_data_loader = DataLoader(log_file_paths, chunk_size=1000)\n",
    "test_data_loader = DataLoader(log_file_paths, chunk_size=1000)\n",
    "\n",
    "# # Step 3: Iterate over the data_loader to process the files in chunks\n",
    "# try:\n",
    "#     for current_file_idx, chunk in data_loader:\n",
    "#         print(f\"Processing chunk of size {len(chunk)} for the file {current_file_idx}:\")\n",
    "#         # Here you can process each chunk of log lines\n",
    "#         for line in chunk:\n",
    "#             print(line)  # Or, do some further processing on each log line\n",
    "#         break  # Remove this line to process all chunks\n",
    "\n",
    "# except StopIteration:\n",
    "#     print(\"All log files have been processed.\")\n",
    "\n",
    "# # Step 4: Close the DataLoader after processing is complete\n",
    "# data_loader.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.file_persistence import FilePersistence\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(message)s')\n",
    "\n",
    "\n",
    "class MultiLevelDrainTrainer:\n",
    "    def __init__(self, train_data_loader, test_data_loader, base_config_path, levels, trained_models_dir):\n",
    "        \"\"\"\n",
    "        Initialize the MultiLevelDrainTrainer.\n",
    "\n",
    "        :param data_loader: An iterable DataLoader to read log lines.\n",
    "        :param base_config_path: Path to the base configuration file for Drain.\n",
    "        :param levels: List of dictionaries defining hyper-parameters for each level.\n",
    "        \"\"\"\n",
    "        self.train_data_loader = train_data_loader\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.base_config_path = base_config_path\n",
    "        self.levels = levels\n",
    "        self.models = []\n",
    "        self.statistics = []  # Store statistics for each level\n",
    "        self.trained_models_dir = trained_models_dir\n",
    "\n",
    "    def _load_config(self, level_params):\n",
    "        \"\"\"\n",
    "        Load the base configuration and apply level-specific hyper-parameters.\n",
    "\n",
    "        :param level_params: Dictionary of hyper-parameters for this level.\n",
    "        :return: A configured TemplateMinerConfig instance.\n",
    "        \"\"\"\n",
    "        config = TemplateMinerConfig()\n",
    "        config.load(self.base_config_path)\n",
    "        for param, value in level_params.items():\n",
    "            setattr(config, param, value)\n",
    "        return config\n",
    "\n",
    "    def train_level(self, level_idx, config):\n",
    "        \"\"\"\n",
    "        Train a single level of Drain with the given configuration.\n",
    "\n",
    "        :param level_idx: Index of the level being trained.\n",
    "        :param config: Configured TemplateMinerConfig instance.\n",
    "        :return: A trained TemplateMiner instance.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Training Drain model for Level {level_idx} with config: {config.__dict__}\")\n",
    "        persistence = FilePersistence(os.path.join(self.trained_models_dir, f\"state_level_{level_idx}.bin\"))\n",
    "        template_miner = TemplateMiner(persistence, config)\n",
    "\n",
    "        # for file_idx, chunk in self.train_data_loader:\n",
    "        #     for line in chunk:\n",
    "        #         line = line.strip()\n",
    "        #         template_miner.add_log_message(line)\n",
    "                # masked_message = template_miner.masker.mask(line)\n",
    "                # tokens = template_miner.drain.get_content_as_tokens(masked_message)\n",
    "                # print(f\"Original: {line}\")\n",
    "                # print(f\"Masked: {\" \".join(tokens)}\")\n",
    "\n",
    "        return template_miner\n",
    "\n",
    "    def collect_statistics(self, level_idx, model):\n",
    "        \"\"\"\n",
    "        Collect frequency and rarity statistics for templates after training is complete.\n",
    "\n",
    "        :param level_idx: Index of the level being analyzed.\n",
    "        :param model: Trained TemplateMiner instance.\n",
    "        :return: A list of statistics for each template.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Collecting statistics for Level {level_idx}...\")\n",
    "        stats = {}\n",
    "        total_lines = 0\n",
    "        template_id_template_map = {}\n",
    "        total_files = set()\n",
    "        # Reprocess training data to collect statistics\n",
    "        for file_idx, chunk in self.test_data_loader:\n",
    "            total_files.add(file_idx)\n",
    "            if file_idx not in stats:\n",
    "                stats[file_idx] = {}\n",
    "\n",
    "            for line in chunk:\n",
    "                line = line.strip()\n",
    "                cluster = model.match(line)\n",
    "                if cluster is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    cluster_id = cluster.cluster_id\n",
    "                    template = cluster.get_template()\n",
    "                    if cluster_id not in template_id_template_map:\n",
    "                        template_id_template_map[cluster_id] = {\"template\": template, \"count\": 0, \"files\": set()}\n",
    "                    template_id_template_map[cluster_id][\"count\"] += 1\n",
    "                    template_id_template_map[cluster_id][\"files\"].add(file_idx)\n",
    "                    if template not in stats[file_idx]:\n",
    "                        stats[file_idx][template] = 0\n",
    "                    stats[file_idx][template] += 1\n",
    "                total_lines += 1\n",
    "\n",
    "        for template_id, template_stats in template_id_template_map.items():\n",
    "            try:\n",
    "                template = template_stats[\"template\"]\n",
    "                template_stats[\"occurrence_ratio\"] = len(template_stats[\"files\"]) / len(total_files)\n",
    "                template_stats[\"average_frequency_per_file\"] = template_stats[\"count\"] / len(total_files)\n",
    "\n",
    "                # Compute Log-Likelihood Ratio (LLR)\n",
    "                p_obs = template_stats[\"count\"] / total_lines\n",
    "                p_exp = 1 / len(template_id_template_map)\n",
    "                template_stats[\"log_likelihood_ratio\"] = template_stats[\"count\"] * math.log(p_obs / p_exp) if p_obs > 0 else 0\n",
    "\n",
    "                probabilities = []\n",
    "                for idx in stats:\n",
    "                    if stats[idx][template]:\n",
    "                        probabilities.append(stats[idx][template])\n",
    "                sum_probabilities = sum(probabilities)\n",
    "                probabilities = [p / sum_probabilities for p in probabilities]\n",
    "                entropy = -sum(p * math.log(p) for p in probabilities if p > 0)\n",
    "                template_stats[\"entropy\"] = entropy\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        self.statistics.append({\"level\": level_idx, \"stats\": template_id_template_map})\n",
    "        return stats\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the Drain models for all levels and collect statistics.\n",
    "        \"\"\"\n",
    "        for level_idx, level_params in enumerate(self.levels, start=1):\n",
    "            config = self._load_config(level_params)\n",
    "            model = self.train_level(level_idx, config)\n",
    "            self.models.append(model)\n",
    "            self.collect_statistics(level_idx, model)\n",
    "\n",
    "    def summarize_models(self):\n",
    "        \"\"\"\n",
    "        Summarize the clusters for all trained models.\n",
    "        \"\"\"\n",
    "        for level_idx, model in enumerate(self.models, start=1):\n",
    "            logger.info(f\"--- Summary for Level {level_idx} ---\")\n",
    "            sorted_clusters = sorted(model.drain.clusters, key=lambda c: c.size, reverse=True)\n",
    "            for cluster in sorted_clusters[:10]:  # Show top 10 clusters\n",
    "                logger.info(cluster)\n",
    "            print(f\"Prefix Tree for Level {level_idx}:\")\n",
    "            model.drain.print_tree()\n",
    "            model.profiler.report(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataloader import DataLoader\n",
    "\n",
    "base_config_path = \"drain3.ini\"\n",
    "trained_models_dir = \"trained_models\"\n",
    "os.makedirs(trained_models_dir, exist_ok=True)\n",
    "levels = [\n",
    "    # {\"max_depth\": 4, \"max_children\": 100},  # Coarse-grained\n",
    "    # {\"max_depth\": 6, \"max_children\": 50},   # Medium-grained\n",
    "    {\"depth\": 8, \"sim_th\": 0.4}    # Fine-grained\n",
    "]\n",
    "\n",
    "# Train Multi-Level Drain\n",
    "trainer = MultiLevelDrainTrainer(train_data_loader, test_data_loader, base_config_path, levels, trained_models_dir)\n",
    "trainer.train()\n",
    "\n",
    "# Summarize Results\n",
    "# trainer.summarize_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save statistics to a CSV file\n",
    "csv_file_path = \"statistics.csv\"\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"level\", \"template_id\", \"template_text\", \"occurrence_ratio\", \"average_frequency_per_file\", \"log_likelihood_ratio\", \"entropy\"])\n",
    "    \n",
    "    # Write the data\n",
    "    for level_stats in trainer.statistics:\n",
    "        level = level_stats[\"level\"]\n",
    "        template_id_template_map = level_stats[\"stats\"]\n",
    "        for template_id, template_stats in template_id_template_map.items():\n",
    "            writer.writerow([\n",
    "                level,\n",
    "                template_id,\n",
    "                template_stats[\"template\"],\n",
    "                template_stats[\"occurrence_ratio\"],\n",
    "                template_stats[\"average_frequency_per_file\"],\n",
    "                template_stats[\"log_likelihood_ratio\"],\n",
    "                template_stats[\"entropy\"]\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level_stats in trainer.statistics:\n",
    "    level = level_stats[\"level\"]\n",
    "    template_id_template_map = level_stats[\"stats\"]\n",
    "    for template_id, template_stats in template_id_template_map.items():\n",
    "        template_stats[\"files\"] = list(template_stats[\"files\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path to save the JSON data\n",
    "file_path = \"statistics.json\"\n",
    "\n",
    "# Save the statistics to a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(trainer.statistics, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(template_stats, occurrence_threshold = 0.1, frequency_threshold = 10, log_likelihood_threshold = 0, entropy_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    Classifies based on the given parameters and thresholds.\n",
    "    \n",
    "    Args:\n",
    "    - template_stats (dict): A dictionary containing the following keys:\n",
    "        - occurrence_ratio (float)\n",
    "        - average_frequency_per_file (float)\n",
    "        - log_likelihood_ratio (float)\n",
    "        - entropy (float)\n",
    "    - occurrence_threshold (float): Threshold for occurrence ratio.\n",
    "    - frequency_threshold (float): Threshold for average frequency per file.\n",
    "    - log_likelihood_threshold (float): Threshold for log likelihood ratio.\n",
    "    - entropy_threshold (float): Threshold for entropy.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the conditions are met, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract values from template_stats\n",
    "    occurrence_ratio = template_stats[\"occurrence_ratio\"]\n",
    "    average_frequency_per_file = template_stats[\"average_frequency_per_file\"]\n",
    "    log_likelihood_ratio = template_stats[\"log_likelihood_ratio\"]\n",
    "    entropy = template_stats[\"entropy\"]\n",
    "    \n",
    "    # Check the conditions: occurrence_ratio, average_frequency_per_file, log_likelihood_ratio should be lower\n",
    "    # entropy should be higher\n",
    "    # if (occurrence_ratio < occurrence_threshold and \n",
    "    #     average_frequency_per_file < frequency_threshold and \n",
    "    #     log_likelihood_ratio < log_likelihood_threshold and \n",
    "    #     entropy > entropy_threshold):\n",
    "    #     return True\n",
    "    # else:\n",
    "    #     return False\n",
    "    \n",
    "    if (average_frequency_per_file < frequency_threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# Define the thresholds for classification\n",
    "occurrence_threshold = 0.1\n",
    "frequency_threshold = 10\n",
    "log_likelihood_threshold = 0\n",
    "entropy_threshold = 0.5\n",
    "\n",
    "anomaly_markings = {}\n",
    "\n",
    "for level_stats in trainer.statistics:\n",
    "    level = level_stats[\"level\"]\n",
    "    template_id_template_map = level_stats[\"stats\"]\n",
    "    for template_id, template_stats in template_id_template_map.items():\n",
    "        template_stats[\"is_anomaly\"] = classifier(template_stats, occurrence_threshold, frequency_threshold, log_likelihood_threshold, entropy_threshold)\n",
    "        anomaly_markings[template_stats[\"template\"]] = template_stats[\"is_anomaly\"]\n",
    "\n",
    "# Specify the file path to save the JSON data\n",
    "file_path = \"anomaly_markings.json\"\n",
    "# Save the statistics to a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(anomaly_markings, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"statistics.json\"\n",
    "\n",
    "# # Load the statistics from the JSON file\n",
    "# with open(file_path, 'r') as json_file:\n",
    "#     trainer.statistics = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Drain3 template miner\n",
      "Checking for saved state\n",
      "Restored 2584 clusters built from 9949358 messages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22395/3834819801.py:96: DeprecationWarning: The default datetime adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.executemany('''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process the log files: 952.66 seconds\n",
      "Number of lines processed: 10000000\n",
      "Processing speed: 10496.94 lines per second\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "base_config_path = \"drain3.ini\"\n",
    "trained_models_dir = \"trained_models\"\n",
    "level_idx = 1\n",
    "log_dir = \"logs\"\n",
    "\n",
    "\n",
    "# Step 1: Create a DataLoader instance\n",
    "log_file_paths = [os.path.join(log_dir, file) for file in os.listdir(log_dir) if file.endswith('.log')]\n",
    "data_loader = DataLoader(log_file_paths, chunk_size=1000)\n",
    "\n",
    "# Step 2: Read the anomaly_markings.json file\n",
    "with open('anomaly_markings.json', 'r') as f:\n",
    "    anomaly_markings = json.load(f)\n",
    "\n",
    "# Step 3: Get the cluster templates using the Drain model\n",
    "config = TemplateMinerConfig()\n",
    "config.load(base_config_path)\n",
    "\n",
    "persistence = FilePersistence(os.path.join(trained_models_dir, f\"state_level_{level_idx}.bin\"))\n",
    "template_miner = TemplateMiner(persistence, config)\n",
    "\n",
    "# Step 4: Check if the template is an anomaly using the anomaly markings\n",
    "def is_anomaly(template):\n",
    "    return anomaly_markings.get(template, False)\n",
    "\n",
    "# Step 5: Split the log line into pieces\n",
    "def split_log_line(line):\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}),\\s+(Info|Error|Warning|Debug)\\s+(\\w+)\\s+(.*)'\n",
    "    match = re.match(pattern, line)\n",
    "    if match:\n",
    "        timestamp, log_level, component, message = match.groups()\n",
    "        return timestamp, log_level, component, message\n",
    "    else:\n",
    "        return None, None, None, None\n",
    "\n",
    "# Step 6: Store anomalies in SQLite3 database\n",
    "conn = sqlite3.connect('anomalies.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS anomalies (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT,\n",
    "        log_level TEXT,\n",
    "        component TEXT,\n",
    "        message TEXT,\n",
    "        template TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        log_line TEXT,\n",
    "        incident_time TIMESTAMP,\n",
    "        cpu_usage REAL,\n",
    "        memory_usage REAL,\n",
    "        criticality INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "line_count = 0\n",
    "data_to_insert = []\n",
    "prev_timestamp = None\n",
    "for file_idx, chunk in data_loader:\n",
    "    for line in chunk:\n",
    "        line_count += 1\n",
    "        line = line.strip()\n",
    "        cluster = template_miner.match(line)\n",
    "        if cluster is not None:\n",
    "            template = cluster.get_template()\n",
    "            if \"error\" in line or \"Error\" in line or \"ERROR\" in line or is_anomaly(template):\n",
    "                if is_anomaly(template):\n",
    "                    critical = 2\n",
    "                else:\n",
    "                    critical = 1\n",
    "                timestamp, log_level, component, message = split_log_line(line)\n",
    "                # Convert timestamp to a datetime object\n",
    "                if timestamp:\n",
    "                    incident_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "                    prev_timestamp = timestamp\n",
    "                else:\n",
    "                    timestamp = prev_timestamp\n",
    "                    message = line\n",
    "\n",
    "                # Filter from the DB if needed (example query)\n",
    "                # cursor.execute('SELECT * FROM anomalies WHERE incident_time > ?', (incident_time,))\n",
    "                # if incident_time:\n",
    "                data_to_insert.append((timestamp, log_level, component, message, template, line, incident_time, critical))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Insert all data at once\n",
    "cursor.executemany('''\n",
    "    INSERT INTO anomalies (timestamp, log_level, component, message, template, log_line, incident_time, criticality)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "''', data_to_insert)\n",
    "\n",
    "print(f\"Time taken to process the log files: {end_time - start_time:.2f} seconds\\nNumber of lines processed: {line_count}\")\n",
    "print(f\"Processing speed: {line_count / (end_time - start_time):.2f} lines per second\")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('anomalies.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to fetch the required data\n",
    "query = '''\n",
    "    SELECT timestamp, message, template, criticality, cpu_usage, memory_usage\n",
    "    FROM anomalies\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Convert the data to a list of dictionaries\n",
    "data = []\n",
    "for row in rows:\n",
    "    data.append({\n",
    "        \"timestamp\": row[0],\n",
    "        \"message\": row[1],\n",
    "        \"template\": row[2],\n",
    "        \"criticality\": row[3],\n",
    "        \"cpu_usage\": row[4],\n",
    "        \"memory_usage\": row[5]\n",
    "    })\n",
    "\n",
    "\n",
    "filtered_data = []\n",
    "template_count = {}\n",
    "\n",
    "for entry in data:\n",
    "    template = entry[\"template\"]\n",
    "    criticality = entry[\"criticality\"]\n",
    "    \n",
    "    if criticality == 1:\n",
    "        if template not in template_count:\n",
    "            template_count[template] = 0\n",
    "        if template_count[template] < 5:\n",
    "            filtered_data.append(entry)\n",
    "            template_count[template] += 1\n",
    "    else:\n",
    "        filtered_data.append(entry)\n",
    "\n",
    "data = filtered_data\n",
    "\n",
    "# Convert the list of dictionaries to JSON\n",
    "json_data = json.dumps(data, indent=4)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Print the JSON data\n",
    "print(len(data))\n",
    "\n",
    "# Specify the file path to save the JSON data\n",
    "file_path = \"metric_data.json\"\n",
    "# Save the statistics to a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Load the updated metric data from the JSON file\n",
    "with open('updated_metric_data.json', 'r') as json_file:\n",
    "    updated_data = json.load(json_file)\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('anomalies.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Update the database for criticality 2 based on timestamp and message\n",
    "for entry in updated_data:\n",
    "    if entry[\"criticality\"] == 2:\n",
    "        cursor.execute('''\n",
    "            UPDATE anomalies\n",
    "            SET cpu_usage = ?, memory_usage = ?\n",
    "            WHERE timestamp = ? AND message = ? AND criticality = 2\n",
    "        ''', (entry[\"cpu_usage\"], entry[\"memory_usage\"], entry[\"timestamp\"], entry[\"message\"]))\n",
    "\n",
    "# Calculate average cpu_usage and memory_usage for criticality 1 by template\n",
    "template_stats = {}\n",
    "for entry in updated_data:\n",
    "    if entry[\"criticality\"] == 1:\n",
    "        template = entry[\"template\"]\n",
    "        if template not in template_stats:\n",
    "            template_stats[template] = {\"cpu_usage\": 0, \"memory_usage\": 0, \"count\": 0}\n",
    "        template_stats[template][\"cpu_usage\"] += entry[\"cpu_usage\"]\n",
    "        template_stats[template][\"memory_usage\"] += entry[\"memory_usage\"]\n",
    "        template_stats[template][\"count\"] += 1\n",
    "\n",
    "# Update the database for criticality 1 with average values by template\n",
    "for template, stats in template_stats.items():\n",
    "    avg_cpu_usage = stats[\"cpu_usage\"] / stats[\"count\"]\n",
    "    avg_memory_usage = stats[\"memory_usage\"] / stats[\"count\"]\n",
    "    cursor.execute('''\n",
    "        UPDATE anomalies\n",
    "        SET cpu_usage = ?, memory_usage = ?\n",
    "        WHERE template = ? AND criticality = 1\n",
    "    ''', (avg_cpu_usage, avg_memory_usage, template))\n",
    "\n",
    "# Commit the changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4589/947702311.py:30: DeprecationWarning: The default datetime adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute('''\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# Read the JSON file\n",
    "with open('syn_data.json', 'r') as json_file:\n",
    "    syn_data = json.load(json_file)\n",
    "\n",
    "# Extract the logs under the key \"failures\"\n",
    "failure_logs = syn_data.get('failures', [])\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('anomalies.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Iterate over each failure log and extract the sub logs\n",
    "for failure in failure_logs:\n",
    "    logs = failure.get('logs', [])\n",
    "    for log in logs:\n",
    "        # print(log)\n",
    "        # Extract the required fields from the log\n",
    "        timestamp = log.get('timestamp')\n",
    "        log_level = log.get('log_level')\n",
    "        message = log.get('message')\n",
    "        cpu_usage = log.get('cpu_usage')\n",
    "        memory_usage = log.get('memory_usage')\n",
    "        template = log.get('drain_template')\n",
    "        criticality = 2\n",
    "\n",
    "        incident_time = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Insert the log into the database\n",
    "        cursor.execute('''\n",
    "            INSERT INTO anomalies (timestamp, log_level, component, message, template, log_line, incident_time, cpu_usage, memory_usage, criticality)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (timestamp, log_level, 'Error', message, template, log['whole_log'], incident_time, cpu_usage, memory_usage, criticality))\n",
    "\n",
    "# Commit the changes and close the database connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023-07-25 14:45:22, Error   APP  Memory access violation in module DEF at address 0x7FFEB432.', '2023-07-25 14:45:45, Warning   APP  Application encountering frequent memory faults.', '2023-07-25 14:46:05, Error   APP  Unhandled exception due to memory read error in process PID 3421.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4589/430452638.py:23: DeprecationWarning: The default datetime adapter is deprecated as of Python 3.12; see the sqlite3 documentation for suggested replacement recipes\n",
      "  cursor.execute(query, (start_time, end_time))\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def fetch_logs(incident_time):\n",
    "    \"\"\"\n",
    "    Fetch log lines within a 2-hour range around the given incident time.\n",
    "\n",
    "    :param incident_time: The incident time as a datetime object.\n",
    "    :return: A list of log lines within the time range.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the time range\n",
    "    start_time = incident_time - timedelta(hours=2)\n",
    "    end_time = incident_time + timedelta(hours=2)\n",
    "\n",
    "    # Query to fetch the log lines within the time range\n",
    "    query = '''\n",
    "        SELECT timestamp, log_level, component, message, template, log_line\n",
    "        FROM anomalies\n",
    "        WHERE incident_time BETWEEN ? AND ?\n",
    "        ORDER BY incident_time\n",
    "    '''\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query, (start_time, end_time))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Convert the data to a list of dictionaries\n",
    "    log_lines = []\n",
    "    for row in rows:\n",
    "        log_lines.append({\n",
    "            \"timestamp\": row[0],\n",
    "            \"log_level\": row[1],\n",
    "            \"component\": row[2],\n",
    "            \"message\": row[3],\n",
    "            \"template\": row[4],\n",
    "            \"log_line\": row[5]\n",
    "        })\n",
    "\n",
    "    logs = [log[\"log_line\"] for log in log_lines]\n",
    "\n",
    "    return logs\n",
    "\n",
    "# Example usage\n",
    "incident_time = datetime.strptime('2023-07-25 14:46:05', '%Y-%m-%d %H:%M:%S')\n",
    "logs = fetch_logs(incident_time)\n",
    "print(logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
